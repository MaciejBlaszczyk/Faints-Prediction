{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1'\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "from matplotlib import pyplot as plt\n",
    "plt.ioff()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, TimeDistributed, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/OPTIMUSPRIME/Desktop/Studia/Magisterka/Faints-Prediction/\"\n",
    "BP_filename = \"BP.csv\"\n",
    "HR_filename = \"HR.csv\"\n",
    "\n",
    "train_indices =  [str(indx[0]) for indx in pd.read_csv(path + \"DATA/training_set.txt\", header=None).values.tolist()]\n",
    "test_indices =  [str(indx[0]) for indx in pd.read_csv(path + \"DATA/test_set.txt\", header=None).values.tolist()]\n",
    "validation_indices =  [str(indx[0]) for indx in pd.read_csv(path + \"DATA/validation_set.txt\", header=None).values.tolist()]\n",
    "all_indices = train_indices + test_indices + validation_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift(xs, n):\n",
    "    if n == 0:\n",
    "        return xs\n",
    "    elif n > 0:\n",
    "        return np.concatenate((np.full(n, np.nan), xs[:-n]))\n",
    "    else:\n",
    "        return np.concatenate((xs[-n:], np.full(-n, np.nan)))\n",
    "\n",
    "    \n",
    "def series_to_supervised(share_prices, timestamps, input_time_steps, dropnan=True):\n",
    "    share_prices_df = pd.DataFrame(share_prices)\n",
    "    timestamps_df = pd.DataFrame(timestamps)\n",
    "    share_prices_timeseries = list()\n",
    "    timestamps_timeseries = list()\n",
    "    \n",
    "    for i in range(input_time_steps-1, -1, -1):\n",
    "        share_prices_timeseries.append(share_prices_df.shift(i))\n",
    "        timestamps_timeseries.append(timestamps_df.shift(i))\n",
    "    \n",
    "    aggregated_share_prices = pd.concat(share_prices_timeseries, axis=1)\n",
    "    aggregated_timestamps = pd.concat(timestamps_timeseries, axis=1)\n",
    "    \n",
    "    if dropnan:\n",
    "        aggregated_share_prices.dropna(inplace=True)\n",
    "        aggregated_timestamps.dropna(inplace=True)\n",
    "\n",
    "    aggregated_timestamps = aggregated_timestamps.values\n",
    "    aggregated_share_prices = aggregated_share_prices.values\n",
    "    \n",
    "    not_overlapping_indexes = range(0, \n",
    "                                    len(aggregated_share_prices), \n",
    "                                    input_time_steps)\n",
    "    \n",
    "    aggregated_timestamps = aggregated_timestamps[not_overlapping_indexes]\n",
    "    aggregated_share_prices = aggregated_share_prices[not_overlapping_indexes]\n",
    "    return aggregated_share_prices, aggregated_timestamps\n",
    " \n",
    "\n",
    "def split(BP_data, HR_data, col, time_steps):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    BP_supervised, HR_supervised = series_to_supervised(BP_data[col], HR_data[col], time_steps)\n",
    "    for BP_interval, HR_interval in zip(BP_supervised, HR_supervised):\n",
    "        BP_HR_interval = []\n",
    "        for BP_time_step, HR_time_step in zip(BP_interval, HR_interval):\n",
    "            BP_HR_interval.append([BP_time_step, HR_time_step])\n",
    "        X.append(BP_HR_interval)\n",
    "        if labels[col] == 'Synkope': label = [0., 1.] \n",
    "        else: label = [1., 0.]\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def split_df(BP_data, HR_data, time_steps):\n",
    "    X = np.array([])\n",
    "    y = np.array([])\n",
    "    for col in BP_data:\n",
    "        X_single, y_single = split(BP_data, HR_data, col, time_steps)\n",
    "        if X.size == 0:\n",
    "            X = X_single\n",
    "            y = y_single\n",
    "        else:\n",
    "            X = np.concatenate((X, X_single))\n",
    "            y = np.concatenate((y, y_single))\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BP_data = pd.read_csv(path + \"DATA/\" + BP_filename, low_memory=False)[all_indices]\n",
    "HR_data = pd.read_csv(path + \"DATA/\" + HR_filename, low_memory=False)[all_indices]\n",
    "labels = {col: BP_data[col].iloc[0] for col in BP_data}\n",
    "\n",
    "BP_max_value = BP_data.iloc[1:].astype(np.float32).max().max()\n",
    "BP_min_value = BP_data.iloc[1:].astype(np.float32).min().min()\n",
    "HR_max_value = HR_data.iloc[1:].astype(np.float32).max().max()\n",
    "HR_min_value = HR_data.iloc[1:].astype(np.float32).min().min()\n",
    "\n",
    "BP_scaler = MinMaxScaler().fit(np.array([BP_min_value, BP_max_value]).reshape(-1,1))\n",
    "HR_scaler = MinMaxScaler().fit(np.array([HR_min_value, HR_max_value]).reshape(-1,1))\n",
    "\n",
    "BP_data_scaled = BP_data.iloc[1:].astype(np.float32).copy()\n",
    "BP_data_scaled[all_indices] = BP_scaler.transform(BP_data_scaled[all_indices])\n",
    "\n",
    "HR_data_scaled = HR_data.iloc[1:].astype(np.float32).copy()\n",
    "HR_data_scaled[all_indices] = HR_scaler.transform(HR_data_scaled[all_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BP_data_scaled = BP_data_scaled.rolling(50).mean()\n",
    "# HR_data_scaled = HR_data_scaled.rolling(50).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Some Unrelevant Data From The Beginning To Improve Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_to_drop = 0.0\n",
    "BP_data_scaled_trimmed = BP_data_scaled.copy()\n",
    "HR_data_scaled_trimmed = HR_data_scaled.copy()\n",
    "for col in BP_data_scaled_trimmed:\n",
    "    n_rows_to_drop = int(part_to_drop * np.count_nonzero(~np.isnan(BP_data_scaled_trimmed[col])))\n",
    "\n",
    "    BP_data_scaled_trimmed[col] = shift(BP_data_scaled_trimmed[col], -n_rows_to_drop)\n",
    "    HR_data_scaled_trimmed[col] = shift(HR_data_scaled_trimmed[col], -n_rows_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = [32] \n",
    "# learing_rates = [0.0005, 0.001, 0.005, 0.01]\n",
    "learning_rates = [0.0005, 0.001, 0.005, 0.01]\n",
    "# r_layers = [0, 2, 3, 4]\n",
    "r_layers = [1, 2]\n",
    "# LSTM_cells = [16, 64, 256]\n",
    "LSTM_cells = [16, 64, 256]\n",
    "# neurons = [5, 20, 50]\n",
    "neurons = [10]\n",
    "# epochs = [1, 2, 3, 4, 5]\n",
    "epochs = [5, 10, 20]\n",
    "# batch_size = [32, 64, 128]\n",
    "batch_size = [128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_timeseries(x):\n",
    "    x = np.array(x)\n",
    "    x = x[~np.isnan(x)]\n",
    "    if x.shape[0] % 32 != 0:\n",
    "        x = x[:-(x.shape[0] % 32)]\n",
    "    return np.array(np.array_split(x, int(len(x)/32)))\n",
    "\n",
    "def build_model(rl, c, n, lr, X_train, y_train):\n",
    "    model = Sequential()\n",
    "    if rl == 1:\n",
    "        model.add(LSTM(c, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "        model.add(BatchNormalization())\n",
    "    elif rl == 2:\n",
    "        model.add(LSTM(c, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LSTM(c))\n",
    "        model.add(BatchNormalization())\n",
    "#     else:\n",
    "#         model.add(LSTM(c, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "#         for _ in range(rl - 2):\n",
    "#             model.add(LSTM(c, return_sequences=True))\n",
    "#         model.add(LSTM(c))\n",
    "    model.add(Dense(n, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "    optimizer = Adam(lr)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def plot_classified_timeseries(BP_data_scaled, HR_data_scaled, y_pred):\n",
    "    BP = sample_timeseries(BP_data_scaled[ind])\n",
    "    HR = sample_timeseries(HR_data_scaled[ind])\n",
    "    print\n",
    "    t = 0\n",
    "    BP_last_elem = None\n",
    "    plt.figure(figsize=(12,8))\n",
    "    for HRv, BPv, y in zip(HR, BP, y_pred):\n",
    "        if BP_last_elem == None:\n",
    "            c = 'r' if y[1] > 0.5 else 'g' \n",
    "            plt.plot(range(t, t + len(HRv)), HRv, color=f'{c}', linestyle='-')\n",
    "            plt.plot(range(t, t + len(BPv)), BPv, color=f'{c}', linestyle='-')\n",
    "            t += len(BPv)\n",
    "        else:\n",
    "            time_range = range(t-1, t + len(BPv)) \n",
    "            BPv = np.concatenate(([BP_last_elem], BPv))\n",
    "            HRv = np.concatenate(([HR_last_elem], HRv))\n",
    "            c = 'r' if y[1] > 0.5 else 'g'\n",
    "            plt.plot(time_range, HRv, color=f'{c}', linestyle='-')\n",
    "            plt.plot(time_range, BPv, color=f'{c}', linestyle='-')\n",
    "            t += len(BPv) - 1\n",
    "        BP_last_elem = BPv[-1]\n",
    "        HR_last_elem = HRv[-1] \n",
    "\n",
    "\n",
    "    plt.title(labels[ind])\n",
    "    plt.ylim((0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130, 32, 2)\n",
      "(129, 32, 2)\n",
      "(83, 32, 2)\n",
      "(85, 32, 2)\n",
      "(92, 32, 2)\n",
      "(118, 32, 2)\n",
      "(74, 32, 2)\n",
      "(132, 32, 2)\n",
      "(156, 32, 2)\n",
      "(91, 32, 2)\n",
      "(111, 32, 2)\n",
      "(145, 32, 2)\n",
      "(146, 32, 2)\n",
      "(47, 32, 2)\n",
      "(202, 32, 2)\n",
      "(127, 32, 2)\n",
      "(169, 32, 2)\n",
      "(136, 32, 2)\n",
      "(130, 32, 2)\n",
      "(129, 32, 2)\n",
      "(83, 32, 2)"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-96124a7051ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m                             \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalidation_indices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                                 \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBP_data_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHR_data_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m                                 \u001b[0my_val_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m    398\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[1;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m                 \u001b[1;31m# newlines imply flush in subprocesses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[1;31m# wake event thread (message content is ignored)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs\\Anaconda3\\lib\\site-packages\\zmq\\sugar\\socket.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    393\u001b[0m                                  copy_threshold=self.copy_threshold)\n\u001b[0;32m    394\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\Programs\\Anaconda3\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "plt.ioff()\n",
    "for ts in time_steps:\n",
    "    X_train, y_train = split_df(BP_data_scaled_trimmed[train_indices], HR_data_scaled_trimmed[train_indices], ts)\n",
    "    X_test, y_test = split_df(BP_data_scaled_trimmed[test_indices], HR_data_scaled_trimmed[test_indices], ts)\n",
    "    for rl in r_layers:\n",
    "        for c in LSTM_cells:\n",
    "            for n in neurons: \n",
    "                for lr in learning_rates:\n",
    "                    model = build_model(rl, c, n, lr, X_train, y_train)\n",
    "                    for e in epochs:\n",
    "                        for bs in batch_size:\n",
    "                            hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=e, batch_size=bs, verbose=0)\n",
    "                            acc = int(hist.history['accuracy'][-1]*100)\n",
    "                            val_acc = int(hist.history['val_accuracy'][-1]*100) \n",
    "                            directory = \"LSTM_RESULTS_TRIMMED_NONAVERAGED/\" + f\"ACC{acc}_VALACC{val_acc}_RL{rl}_C{c}_N{n}_E{e}_BS{bs}_LR{lr}\"\n",
    "                            os.mkdir(directory)\n",
    "                            model.save(directory + f\"/model\")\n",
    "                            for ind in validation_indices:                            \n",
    "                                X_val, y_val = split(BP_data_scaled, HR_data_scaled, ind, ts)\n",
    "                                print(.X_val.shape)\n",
    "                                y_val_pred = model.predict(X_val)\n",
    "\n",
    "                                plot_classified_timeseries(BP_data_scaled, HR_data_scaled, y_val_pred)\n",
    "                                plt.savefig(directory + f\"/{ind}.png\")        \n",
    "                                plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
