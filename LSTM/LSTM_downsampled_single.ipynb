{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1'\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, TimeDistributed, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/OPTIMUSPRIME/Desktop/Studia/Magisterka/Faints-Prediction/\"\n",
    "BP_filename = \"BP.csv\"\n",
    "HR_filename = \"HR.csv\"\n",
    "\n",
    "train_indices =  [str(indx[0]) for indx in pd.read_csv(path + \"DATA/training_set.txt\").values.tolist()]\n",
    "test_indices =  [str(indx[0]) for indx in pd.read_csv(path + \"DATA/test_set.txt\").values.tolist()]\n",
    "validation_indices =  [str(indx[0]) for indx in pd.read_csv(path + \"DATA/validation_set.txt\").values.tolist()]\n",
    "all_indices = train_indices + test_indices + validation_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BP_data = pd.read_csv(path + \"DATA/\" + BP_filename, low_memory=False)[all_indices]\n",
    "HR_data = pd.read_csv(path + \"DATA/\" + HR_filename, low_memory=False)[all_indices]\n",
    "labels = {col: BP_data[col].iloc[0] for col in BP_data}\n",
    "\n",
    "BP_max_value = BP_data.iloc[1:].astype(np.float32).max().max()\n",
    "BP_min_value = BP_data.iloc[1:].astype(np.float32).min().min()\n",
    "HR_max_value = HR_data.iloc[1:].astype(np.float32).max().max()\n",
    "HR_min_value = HR_data.iloc[1:].astype(np.float32).min().min()\n",
    "\n",
    "BP_scaler = MinMaxScaler().fit(np.array([BP_min_value, BP_max_value]).reshape(-1,1))\n",
    "HR_scaler = MinMaxScaler().fit(np.array([HR_min_value, HR_max_value]).reshape(-1,1))\n",
    "\n",
    "BP_data_scaled = BP_data.iloc[1:].astype(np.float32).copy()\n",
    "BP_data_scaled[all_indices] = BP_scaler.transform(BP_data_scaled[all_indices])\n",
    "\n",
    "HR_data_scaled = HR_data.iloc[1:].astype(np.float32).copy()\n",
    "HR_data_scaled[all_indices] = HR_scaler.transform(HR_data_scaled[all_indices])\n",
    "\n",
    "def downsample_to(n, df, mean_window):\n",
    "    new_df = pd.DataFrame()\n",
    "    for col in df:\n",
    "        data = df[col][1:]\n",
    "#         data_temp = data.count()\n",
    "        div = int(data.count() / n) - 1 # -1 bo jakies problemy sa ze avr_data po usrednieniu ma dlugosc < 200 i sa ujemne excessPoints\n",
    "        avr_data = data.rolling(mean_window).mean()[::div]\n",
    "        avr_data = [_ for _ in list(avr_data) if np.isnan(_) != True]\n",
    "#         temp = len(avr_data)\n",
    "        excess_points = len(avr_data) - n\n",
    "        avr_data = avr_data[excess_points:]\n",
    "        new_df[col] = pd.Series(avr_data)\n",
    "#         print(f\"TS {col} processing. Div: {div}. Excess points: {excess_points}. Dl: {temp}. data len: {data_temp}\")\n",
    "    return new_df\n",
    "\n",
    "BP_data_scaled = downsample_to(200, BP_data_scaled, 50)\n",
    "HR_data_scaled = downsample_to(200, HR_data_scaled, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(BP_data, HR_data):\n",
    "    X = []\n",
    "    y = []\n",
    "    for col in BP_data:\n",
    "        BP_HR_interval = []\n",
    "        for BP_interval, HR_interval in zip(BP_data[col], HR_data[col]):\n",
    "            BP_HR_interval.append([BP_interval, HR_interval])\n",
    "        X.append(BP_HR_interval)\n",
    "        if labels[col] == 'Synkope': label = [0., 1.] \n",
    "        else: label = [1., 0.]\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = split(BP_data_scaled[train_indices], HR_data_scaled[train_indices])\n",
    "X_test, y_test = split(BP_data_scaled[test_indices], HR_data_scaled[test_indices])\n",
    "X_validation, y_validation = split(BP_data_scaled[validation_indices], HR_data_scaled[validation_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0422 00:50:21.155788 20628 deprecation_wrapper.py:119] From C:\\Programs\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0422 00:50:21.196244 20628 deprecation_wrapper.py:119] From C:\\Programs\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0422 00:50:21.205222 20628 deprecation_wrapper.py:119] From C:\\Programs\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0422 00:50:22.411896 20628 deprecation_wrapper.py:119] From C:\\Programs\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0422 00:50:22.445807 20628 deprecation_wrapper.py:119] From C:\\Programs\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "c=64\n",
    "n=50\n",
    "model.add(LSTM(c, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(c, return_sequences=True))\n",
    "model.add(LSTM(c, return_sequences=True))\n",
    "model.add(LSTM(c))\n",
    "model.add(Dense(n, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 200, 64)           17152     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 200, 64)           33024     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200, 64)           33024     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                3250      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 119,576\n",
      "Trainable params: 119,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0422 00:50:22.675969 20628 deprecation.py:323] From C:\\Programs\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0422 00:50:25.555685 20628 deprecation_wrapper.py:119] From C:\\Programs\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 153 samples, validate on 19 samples\n",
      "Epoch 1/5\n",
      "153/153 [==============================] - 4s 29ms/step - loss: 0.6928 - acc: 0.5556 - val_loss: 0.6955 - val_acc: 0.4737\n",
      "Epoch 2/5\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.6902 - acc: 0.5098 - val_loss: 0.6975 - val_acc: 0.4737\n",
      "Epoch 3/5\n",
      "153/153 [==============================] - 1s 10ms/step - loss: 0.6868 - acc: 0.5098 - val_loss: 0.7046 - val_acc: 0.4737\n",
      "Epoch 4/5\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 0.6826 - acc: 0.5098 - val_loss: 0.7138 - val_acc: 0.4737\n",
      "Epoch 5/5\n",
      "153/153 [==============================] - 2s 10ms/step - loss: 0.6753 - acc: 0.5229 - val_loss: 0.7166 - val_acc: 0.4737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5465911 , 0.4534089 ],\n",
       "       [0.5048299 , 0.4951701 ],\n",
       "       [0.55375636, 0.44624364],\n",
       "       [0.49757397, 0.502426  ],\n",
       "       [0.5271346 , 0.47286537],\n",
       "       [0.55761594, 0.44238406],\n",
       "       [0.5493014 , 0.4506986 ],\n",
       "       [0.56306565, 0.43693438],\n",
       "       [0.49305546, 0.50694454],\n",
       "       [0.5080884 , 0.4919116 ],\n",
       "       [0.5711759 , 0.42882404],\n",
       "       [0.540348  , 0.45965198],\n",
       "       [0.5074196 , 0.49258035],\n",
       "       [0.52853125, 0.4714687 ],\n",
       "       [0.56322014, 0.4367798 ],\n",
       "       [0.5405539 , 0.459446  ],\n",
       "       [0.60848737, 0.39151263]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=128, verbose=1)\n",
    "model.predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['158',\n",
       " '460',\n",
       " '606',\n",
       " '320',\n",
       " '1',\n",
       " '658',\n",
       " '559',\n",
       " '92',\n",
       " '538',\n",
       " '675',\n",
       " '419',\n",
       " '295',\n",
       " '393',\n",
       " '409',\n",
       " '501',\n",
       " '375',\n",
       " '286']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_indices\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
